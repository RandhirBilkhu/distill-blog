[
  {
    "path": "posts/2020-12-22-monte-carlo-simulation-in-python-part-2/",
    "title": "Monte Carlo Simulation in Python Part 2",
    "description": "Use classes and modules and wrap inside a Python package to make the code in part 1 reuseable.",
    "author": [
      {
        "name": "Randhir Bilkhu",
        "url": {}
      }
    ],
    "date": "2020-12-22",
    "categories": [
      "Python",
      "pandas",
      "numpy",
      "actuarial",
      "catastrophe models"
    ],
    "contents": "\r\nIn part 1 I used a Jupyter notebook to perform step by step calculations in order to generate a YLT. I feel that Jupyter notebooks/R scripts are useful for ad-hoc analysis and getting to an answer quickly. However I don’t think they are the best for making code ready for production.\r\nIn this post, I want to take that code and make it much easier to re-use in the future. I will do this by wrapping up the bulk of the steps inside two classes and putting them into modules\r\nBefore I start I will define some of the terminology.\r\nClasses allow objects to be created, effectively a blueprint for how an object should look and behave. Each object can have attributes and methods which can be utilised. Methods are like functions that belong to the object.\r\nA module is a file containing Python definitions and statements. The file name is the module name with the suffix .py appended.\r\nCreating the classes\r\nI decided to start by creating a class Elt (see below). I am going to use this class to return a parameterised ELT from a raw elt dataframe.\r\nFirstly I start by creating a function called __init__ this method will be called any time an object is created from the class and will allow the class to initialize the attributes of the class Note that inside __init__ I have two arguments, self and pandas_obj. self is a reference to the current instance of the class (note you dont have to use the word ‘self’ but it is the convention. ) The second is pandas_obj as I am going to be passing a dataframe into this class.\r\nSecondly, I define a validate method which will raise an error if the required columns are not in the dataframe.The main method for this class is the function parameterise - inside this function I put all of the calculations to generate the parameterised elt.\r\nAt the end, I also define a property which is the sum of event frequency.\r\n\r\nclass Elt:\r\n    \r\n    def __init__(self,pandas_obj):\r\n        self.df = pandas_obj\r\n        self._validate(pandas_obj)\r\n    \r\n    @staticmethod\r\n    def _validate(obj):\r\n        # verify the minumum required columns are present in the intial dataframe\r\n        if 'id' not in obj.columns or 'rate' not in obj.columns or 'mean' not in obj.columns or 'sdevi' not in obj.columns or 'sdevc' not in obj.columns or 'exp' not in obj.columns:\r\n            raise AttributeError(\"dataframe must have id, rate, mean, sdevi, sdevc and exp columns\")\r\n\r\n    def parameterise(self):\r\n        self.df['mdr'] = self.df['mean']  /self.df['exp']   # calculates the mean damage ratio equivalent to average loss over total exposed\r\n        self.df['sdev'] = self.df['sdevi'] + self.df['sdevc'] # sums up the correlated and independent standard deviations \r\n        self.df['cov'] = self.df['sdev'] /self.df['mean'] # calculates covariance based on total standard deviation\r\n        self.df['alpha'] = (1 - self.df['mdr']) / (self.df['cov']**2 - self.df['mdr']) # generates an alpha parameter for beta distribution\r\n        #alpha is finite <-0 TODO\r\n        self.df['beta'] = (self.df['alpha'] * (1 - self.df['mdr'])) / self.df['mdr']  # generates a beta parameter for beta distribution\r\n        self.df['rand_num'] = self.df['rate'] / self.df['rate'].sum()  # probability of event occuring = normalised event frequency \r\n        self.df.index += 1 ### want to set index to start from 1 ( so sampling works)\r\n\r\n    @property\r\n    def events(self):\r\n        total = self.df['rate'].sum()\r\n    \r\n\r\nNow I can write a class for everything needed for the YLT:\r\n\r\nimport numpy as np\r\n\r\nclass Ylt:\r\n    \"\"\"\r\n    generates YLT\r\n    \"\"\"\r\n    seed = np.random.seed(42)\r\n    \r\n    def __init__(self,pandas_obj):\r\n        self.df = pandas_obj\r\n\r\n    def generate_ylt(self, sims=10):\r\n\r\n        num_events = np.random.poisson(self.df['rate'].sum(), sims) \r\n\r\n        sample_ids = np.random.choice( a = self.df['id'] , size = num_events.sum() , replace= True, p = self.df['rand_num'] ) \r\n        self.df = self.df[['id', 'alpha','beta','exp']].iloc[self.df.index.get_indexer(sample_ids)] ### this took some effort! \r\n        self.df['severity_mdr'] = self.df.apply( lambda x: np.random.beta( x['alpha'] , x['beta']  ) , axis=1 ) ### use apply with axis =1 to use function on each row\r\n        self.df['severity'] = self.df['severity_mdr'] * self.df['exp'] ### this gives us severity for each event\r\n\r\n        year = np.arange(1, sims + 1, 1) # start (included): 0, stop (excluded): 10, step:1\r\n        all_years = pd.DataFrame(year , columns=['year'])\r\n\r\n        self.df['year'] = np.repeat(year, num_events)\r\n        self.df = self.df[['year', 'severity']]\r\n        self.df = pd.merge(self.df, all_years, how='right').fillna(0)\r\n\r\n    def oep(self):\r\n        \r\n        rp = pd.DataFrame([10000,5000,1000,500,250,200,100,50, 25,10,5,2], columns=['return_period'])\r\n        rp['ntile'] = 1 - 1 / rp['return_period'] \r\n        return(self.df.groupby(['year'])['severity'].max().quantile(rp['ntile']))\r\n\r\nFirstly I use numpy to set the random seed so that results are reproducible. (Note - would it make a difference inside the init function?)\r\nNext, I define a method generate_ylt which performs the simulation steps in part1 and returns the YLT containing a column with year and column with the loss amount.\r\nLastly I define a method to calculate oep. For the purposes of this exercise I fixed the calculation of OEP at the specified return periods, but I could have passed an additional argument rp along with self to allow the return periods to be variable.\r\nContaing the modules inside a package\r\nI can define these two classes as modules as elt.py and ylt.py and contain them in a package called elt_py. In order to read the folder as a package an empty .py file with __init__.py is also needed at the folder level.\r\nThere are a few other things to take care of in order to distribute the code as a package but they are outside the scope of this post.\r\n Once I have create the package, I can import the classes from the elt_py package and the process generate a YLT is much simplified. I can generate the oep with just 8 lines of code and the ability to instantiate different instances of ELTs and YLTs make it easy to apply these classes to different ELTs\r\nfrom elt_py.elt import Elt\r\nfrom elt_py.ylt import Ylt\r\n\r\nimport pandas as pd\r\n\r\nX = pd.read_csv(\"example.csv\")\r\n\r\nelt = Elt(X)\r\nelt.parameterise()\r\nylt = Ylt(elt.df)\r\nylt.generate_ylt(sims=2000)\r\n\r\nprint(ylt.oep())\r\nsnippet from VS codeSummary\r\nCreating classes and containing code in modules requires a good deal of extra work over writing an ad-hoc script. However by writing a class and housing the code into modules ( and a package), it makes it much easier to re-use and distribute the code.\r\nOne of the other major benefits would be the ability to incorporate unit testing on the methods being created which I will cover in a future post in detail.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-22-monte-carlo-simulation-in-python-part-2/package.png",
    "last_modified": "2020-12-23T12:05:07+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-18-monte-carlo-simulation-in-python-part-1/",
    "title": "Monte Carlo Simulation in Python Part 1",
    "description": "A step by step walk through of generating simulations from an ELT file using pandas and numpy",
    "author": [
      {
        "name": "Randhir Bilkhu",
        "url": {}
      }
    ],
    "date": "2020-12-18",
    "categories": [
      "Python",
      "pandas",
      "numpy",
      "actuarial",
      "catastrophe models"
    ],
    "contents": "\r\n\r\nContents\r\nA bit about ELT’s\r\nHigh Level Process\r\nStep 0 : Import the libraries\r\nStep 1 : Load the data\r\nStep 2 : Transfom the raw ELT\r\nStep 3 : Simulate Frequency of events by yearStep 4: Draw an Event from the ELT for each event\r\nStep 5 : Simulate event severity\r\nStep 6: Allow for zero event years in the YLT\r\nApplication : Calculate OEP Curve\r\n\r\n\r\nIn order to help improve my use of Python, I decided to try an exercise to simulate loss events from an ELT ( Event loss table). I’ve split this exercise into two parts;\r\nPart 1 is about walking through the steps to perform the calculation. I use a Jupyter notebook to help run through this step by step\r\nPart 2 is about making the code reuseable and robust. I use VSCode to create modules/tests for the differnt elements of the process.\r\nThe focus of these posts are more about the application of Python to achieve this task rather than the underlying theory. Interested readers can refer to this paper by the Casulaty Actuarial society which explains a lot of the underlying concepts.\r\nA bit about ELT’s\r\nELTS are a common output from the RMS property catstrophe model and usually contain at least the following:\r\nEvent ID :Unique identifier of the event\r\nRate : Annual event frequency\r\nMean : Average Loss if the event occurs\r\nSdi : Independent component of the spread of the loss if the event occurs.\r\nSdc : Correlated component of the spread of the loss if the event occurs.\r\nExposure : Total amount of limits exposed to the event (Maximum loss)\r\nWe can use monte carlo techniques to sample events from this table and model loss occurrences over N years - to generate a YLT (Year Loss Table) which can then be used for analysis.\r\nHigh Level Process\r\ngenerate a random sample of events by year assuming a poisson frequency\r\nsample an event from the catalogue in the id and assign to the random sample\r\nfor each event generate a loss amount assuming a beta distribition\r\nmake some final modifications to tidy up the YLT\r\nshow one use case of the YLT.\r\nStep 0 : Import the libraries\r\nAll we need is pandas and numpy - an alternative would be scipy\r\nimport pandas as pd\r\nimport numpy as np\r\nStep 1 : Load the data\r\nI have created a small sample ELT file - typically in reality there can thousands of catalogued events in the table.\r\nelt = pd.read_csv(\"example.csv\")\r\nprint(elt)\r\n   id  rate   mean  sdevi  sdevc      exp\r\n0   1  0.10    500    500    200   100000\r\n1   2  0.10    200    400    100     5000\r\n2   3  0.20    300    200    400    40000\r\n3   4  0.10    100    300    500     4000\r\n4   5  0.20    500    100    200     2000\r\n5   6  0.25    200    200    500    50000\r\n6   7  0.01   1000    500    600   100000\r\n7   8  0.12    250    300    100     5000\r\n8   9  0.14   1000    500    200     6000\r\n9  10  0.00  10000   1000    500  1000000\r\nStep 2 : Transfom the raw ELT\r\nWe need to calculate some additional statistics which will be necessary in order to run the simulation, which can be done using pandas.\r\nI also set the index to start from 1 (Python by default starts from 0), firstly because I’m used to starting from one as an R user and secondly it will make some of the operations further on much easier to manage.\r\nNote the syntax += which in python is equivalent to x = x + 1\r\nelt['mdr'] = elt['mean'] / elt['exp']  # calculates the mean damage ratio equivalent to average loss over total exposed\r\nelt['sdev'] = elt['sdevi'] + elt['sdevc'] # sums up the correlated and independent standard deviations \r\nelt['cov'] = elt['sdev'] /elt['mean'] # calculates covariance based on total standard deviation\r\nelt['alpha'] = (1 - elt['mdr']) / (elt['cov']**2 - elt['mdr']) # generates an alpha parameter for beta distribution\r\n#alpha is finite <-0 TODO\r\nelt['beta'] = (elt['alpha'] * (1 - elt['mdr'])) / elt['mdr']  # generates a beta parameter for beta distribution\r\n\r\nlda = elt['rate'].sum()  # total expected event frequency \r\n\r\nelt['rand_num'] = elt['rate'] / lda # probability of event occuring = normalised event frequency \r\n\r\nelt.index += 1 ### want to set index to start from 1 ( so sampling works)\r\nprint(elt)\r\n    id  rate   mean  sdevi  sdevc      exp       mdr  sdev   cov      alpha  \\\r\n1    1  0.10    500    500    200   100000  0.005000   700  1.40   0.508951   \r\n2    2  0.10    200    400    100     5000  0.040000   500  2.50   0.154589   \r\n3    3  0.20    300    200    400    40000  0.007500   600  2.00   0.248591   \r\n4    4  0.10    100    300    500     4000  0.025000   800  8.00   0.015240   \r\n5    5  0.20    500    100    200     2000  0.250000   300  0.60   6.818182   \r\n6    6  0.25    200    200    500    50000  0.004000   700  3.50   0.081333   \r\n7    7  0.01   1000    500    600   100000  0.010000  1100  1.10   0.825000   \r\n8    8  0.12    250    300    100     5000  0.050000   400  1.60   0.378486   \r\n9    9  0.14   1000    500    200     6000  0.166667   700  0.70   2.577320   \r\n10  10  0.00  10000   1000    500  1000000  0.010000  1500  0.15  79.200000   \r\n\r\n           beta  rand_num  \r\n1    101.281330  0.081967  \r\n2      3.710145  0.081967  \r\n3     32.896890  0.163934  \r\n4      0.594373  0.081967  \r\n5     20.454545  0.163934  \r\n6     20.251837  0.204918  \r\n7     81.675000  0.008197  \r\n8      7.191235  0.098361  \r\n9     12.886598  0.114754  \r\n10  7840.800000  0.000000  \r\nStep 3 : Simulate Frequency of events by year\r\nWe can run the simulation over 100 years. This parameter will need to be much higher in practice in order to provide a reasonable YLT. Especially when using ELTS for Earthquake losses which have very low probabilities of occurence.\r\nI assume frequency is a poisson random variable ( alternate would be negative binomial but would require an extra step to estimate the variance). We set the mean frequency to be equal to the sum of frequencies of each event and then use np.random.poisson to generate a number of events for each year\r\nrandom.seed() is useful to reproduce the data given by a pseudo-random number generator. By re-using a seed value, we can regenerate the same data multiple times as multiple threads are not running. This is helpful for reproducibility purposes\r\nlda = elt['rate'].sum() ### total expected annual event frequency\r\nsims = 100  ## equivalent to number of years we will  simulate \r\nnp.random.seed(42)\r\nnum_events = np.random.poisson(lda, sims) \r\nprint(num_events)\r\n[2 1 0 0 3 2 0 0 1 1 1 0 1 1 2 1 0 3 0 2 1 1 1 1 0 6 0 0 1 0 2 1 1 2 0 4 1\r\n 2 0 1 3 0 3 1 1 0 0 1 2 2 0 0 0 5 4 0 0 3 1 0 0 2 2 2 1 0 0 2 0 1 0 2 2 1\r\n 2 0 1 0 0 2 1 1 1 1 1 2 3 0 0 3 1 4 0 2 0 1 0 2 0 1]\r\nStep 4: Draw an Event from the ELT for each event\r\nI used the normalised frequency and np.random.choice to sample an id for each event.\r\nsample_ids = np.random.choice( a = elt['id'] , size = num_events.sum() , replace= True, p = elt['rand_num'] ) # for each occurrence we sample a loss event from the elt and create an array of ids\r\n\r\nlen(sample_ids) \r\n116\r\nStep 5 : Simulate event severity\r\nI create a new dataframe called sampled_event_loss which containst the alpha, beta parameters and amount exposed,for each event sampled. This required use of the iloc property in pandas which allows subsetting via reference ( the reference was the array of sampled ids)\r\nwe can apply random.beta from numpy to sample a mean damage ratio (mdr) for each event from the distribution. Loss severity can be calculated by multiplying the sampled mdr with the amount exposed.\r\nAlmost there with the YLT! Now just a few modifications are necessary.\r\nsampled_event_loss = elt[['id', 'alpha','beta','exp']].iloc[elt.index.get_indexer(sample_ids)] ### this took some effort! \r\nsampled_event_loss['severity_mdr'] = sampled_event_loss.apply( lambda x: np.random.beta( x['alpha'] , x['beta']  ) , axis=1 ) ### use apply with axis =1 to use function on each row\r\nsampled_event_loss['severity'] = sampled_event_loss['severity_mdr'] * sampled_event_loss['exp'] ### this gives us severity for each event\r\nprint(sampled_event_loss)\r\n    id     alpha       beta    exp  severity_mdr      severity\r\n3    3  0.248591  32.896890  40000  1.108744e-09  4.434975e-05\r\n6    6  0.081333  20.251837  50000  1.641959e-14  8.209797e-10\r\n6    6  0.081333  20.251837  50000  1.544267e-11  7.721333e-07\r\n8    8  0.378486   7.191235   5000  7.498051e-05  3.749025e-01\r\n6    6  0.081333  20.251837  50000  1.355502e-05  6.777512e-01\r\n..  ..       ...        ...    ...           ...           ...\r\n6    6  0.081333  20.251837  50000  4.924032e-07  2.462016e-02\r\n5    5  6.818182  20.454545   2000  2.377589e-01  4.755179e+02\r\n3    3  0.248591  32.896890  40000  4.575936e-04  1.830374e+01\r\n4    4  0.015240   0.594373   4000  6.785872e-22  2.714349e-18\r\n6    6  0.081333  20.251837  50000  1.918026e-11  9.590128e-07\r\n\r\n[116 rows x 6 columns]\r\nStep 6: Allow for zero event years in the YLT\r\nThe YLT needs to be adjusted for the years in which no loss occurred as this currently isnt being reflected in the table. We need to add a row for each year with no loss with 0 for severity. This will help ensure the YLT is accurate. This is a particularly important step when looking at perils with very low frequencies such as Earthquakes.\r\nnum_events\r\n### have years with 0 events this will throw off exceedance probability calculations\r\narray([2, 1, 0, 0, 3, 2, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 0, 3, 0, 2, 1, 1,\r\n       1, 1, 0, 6, 0, 0, 1, 0, 2, 1, 1, 2, 0, 4, 1, 2, 0, 1, 3, 0, 3, 1,\r\n       1, 0, 0, 1, 2, 2, 0, 0, 0, 5, 4, 0, 0, 3, 1, 0, 0, 2, 2, 2, 1, 0,\r\n       0, 2, 0, 1, 0, 2, 2, 1, 2, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 2, 3, 0,\r\n       0, 3, 1, 4, 0, 2, 0, 1, 0, 2, 0, 1])\r\nI used np.arange to generate a sequence from 1 to 100 for each year. Note by default the stop value is excluded hence the need for sims + 1. Next we create a dataframe from that array with the column name year.\r\nFinally we can use np.repeat which will create a feature in sampled_event_loss for the year in which each event occurred.\r\nyear = np.arange(1, sims+1, 1) # start (included): 0, stop (excluded): 10, step:1\r\nall_years = pd.DataFrame(year , columns=['year'])\r\n\r\nsampled_event_loss['year'] = np.repeat(year, num_events)\r\nprint(sampled_event_loss)\r\n    id     alpha       beta    exp  severity_mdr      severity  year\r\n3    3  0.248591  32.896890  40000  1.108744e-09  4.434975e-05     1\r\n6    6  0.081333  20.251837  50000  1.641959e-14  8.209797e-10     1\r\n6    6  0.081333  20.251837  50000  1.544267e-11  7.721333e-07     2\r\n8    8  0.378486   7.191235   5000  7.498051e-05  3.749025e-01     5\r\n6    6  0.081333  20.251837  50000  1.355502e-05  6.777512e-01     5\r\n..  ..       ...        ...    ...           ...           ...   ...\r\n6    6  0.081333  20.251837  50000  4.924032e-07  2.462016e-02    94\r\n5    5  6.818182  20.454545   2000  2.377589e-01  4.755179e+02    96\r\n3    3  0.248591  32.896890  40000  4.575936e-04  1.830374e+01    98\r\n4    4  0.015240   0.594373   4000  6.785872e-22  2.714349e-18    98\r\n6    6  0.081333  20.251837  50000  1.918026e-11  9.590128e-07   100\r\n\r\n[116 rows x 7 columns]\r\nNow we can create a dataframe YLT with just year and severity and use pd.merge to add in the years with 0 loss. Without using fillna() these will appear as NaN in the data frame\r\nWe end up with a dataframe which shows the year in which loss occured and the loss amount. This can now be the basis for applying a reinsurance treaty structure and calculating layered losses.\r\nylt = sampled_event_loss[['year', 'severity']]\r\nylt = pd.merge(ylt, all_years, how='right').fillna(0)\r\n \r\nprint(ylt.to_string()) # allows all records to be displayed\r\n     year      severity\r\n0       1  4.434975e-05\r\n1       1  8.209797e-10\r\n2       2  7.721333e-07\r\n3       3  0.000000e+00\r\n4       4  0.000000e+00\r\n5       5  3.749025e-01\r\n6       5  6.777512e-01\r\n7       5  5.491075e+02\r\n8       6  5.105934e+01\r\n9       6  6.427178e-19\r\n10      7  0.000000e+00\r\n11      8  0.000000e+00\r\n12      9  1.709651e+03\r\n13     10  1.384307e+01\r\n14     11  5.109882e+02\r\n15     12  0.000000e+00\r\n16     13  3.043422e-10\r\n17     14  1.191095e+03\r\n18     15  7.401779e+01\r\n19     15  3.821698e+02\r\n20     16  3.786217e+02\r\n21     17  0.000000e+00\r\n22     18  1.404651e-09\r\n23     18  4.645606e+02\r\n24     18  1.337478e+02\r\n25     19  0.000000e+00\r\n26     20  5.501849e-04\r\n27     20  6.297897e+01\r\n28     21  2.155536e+02\r\n29     22  9.456517e+01\r\n30     23  6.318721e-02\r\n31     24  1.463509e+03\r\n32     25  0.000000e+00\r\n33     26  4.000737e+02\r\n34     26  6.542851e+02\r\n35     26  4.995096e-14\r\n36     26  1.531346e+01\r\n37     26  8.527407e+02\r\n38     26  6.277614e+02\r\n39     27  0.000000e+00\r\n40     28  0.000000e+00\r\n41     29  1.025373e+03\r\n42     30  0.000000e+00\r\n43     31  8.203426e+02\r\n44     31  6.533643e+01\r\n45     32  7.202268e+00\r\n46     33  3.569292e-19\r\n47     34  6.258983e+01\r\n48     34  4.648111e-02\r\n49     35  0.000000e+00\r\n50     36  2.446549e+02\r\n51     36  5.153527e+02\r\n52     36  1.599666e+03\r\n53     36  2.142401e-07\r\n54     37  3.825918e+02\r\n55     38  1.231233e-05\r\n56     38  1.290300e+03\r\n57     39  0.000000e+00\r\n58     40  4.572607e+02\r\n59     41  2.817630e-01\r\n60     41  5.395940e+02\r\n61     41  5.805758e+02\r\n62     42  0.000000e+00\r\n63     43  4.282555e+02\r\n64     43  1.033512e-05\r\n65     43  8.384080e-03\r\n66     44  1.071078e-26\r\n67     45  6.705483e+02\r\n68     46  0.000000e+00\r\n69     47  0.000000e+00\r\n70     48  1.174639e+03\r\n71     49  3.909565e+01\r\n72     49  6.985921e-01\r\n73     50  4.954364e+02\r\n74     50  6.822492e+02\r\n75     51  0.000000e+00\r\n76     52  0.000000e+00\r\n77     53  0.000000e+00\r\n78     54  3.839980e+02\r\n79     54  7.611154e+01\r\n80     54  1.545606e-14\r\n81     54  3.325711e-04\r\n82     54  9.315729e+00\r\n83     55  7.976122e+01\r\n84     55  4.177383e-36\r\n85     55  8.632855e+00\r\n86     55  2.856225e+01\r\n87     56  0.000000e+00\r\n88     57  0.000000e+00\r\n89     58  1.293269e+00\r\n90     58  1.234734e+02\r\n91     58  6.529987e+02\r\n92     59  5.766023e+02\r\n93     60  0.000000e+00\r\n94     61  0.000000e+00\r\n95     62  4.892098e+01\r\n96     62  3.527225e+00\r\n97     63  4.131111e+02\r\n98     63  8.828718e+01\r\n99     64  8.390815e-01\r\n100    64  2.293760e-23\r\n101    65  5.391410e+02\r\n102    66  0.000000e+00\r\n103    67  0.000000e+00\r\n104    68  6.567555e+02\r\n105    68  3.015369e+00\r\n106    69  0.000000e+00\r\n107    70  3.573933e+01\r\n108    71  0.000000e+00\r\n109    72  6.060894e-09\r\n110    72  3.794436e+01\r\n111    73  1.055675e+02\r\n112    73  3.697382e+02\r\n113    74  6.461396e+02\r\n114    75  2.149523e+03\r\n115    75  1.141927e+03\r\n116    76  0.000000e+00\r\n117    77  7.663019e+02\r\n118    78  0.000000e+00\r\n119    79  0.000000e+00\r\n120    80  4.003370e+02\r\n121    80  9.739867e+01\r\n122    81  1.471702e+02\r\n123    82  1.182795e+01\r\n124    83  5.548184e+02\r\n125    84  2.044581e+03\r\n126    85  7.969508e+02\r\n127    86  9.489739e+02\r\n128    86  1.785455e+01\r\n129    87  5.495236e+02\r\n130    87  9.719987e-60\r\n131    87  3.351133e-01\r\n132    88  0.000000e+00\r\n133    89  0.000000e+00\r\n134    90  1.436955e+02\r\n135    90  3.219530e+02\r\n136    90  4.486940e+02\r\n137    91  1.002510e+03\r\n138    92  5.628708e-01\r\n139    92  1.236078e+03\r\n140    92  1.578653e+03\r\n141    92  1.365199e+03\r\n142    93  0.000000e+00\r\n143    94  4.649067e+02\r\n144    94  2.462016e-02\r\n145    95  0.000000e+00\r\n146    96  4.755179e+02\r\n147    97  0.000000e+00\r\n148    98  1.830374e+01\r\n149    98  2.714349e-18\r\n150    99  0.000000e+00\r\n151   100  9.590128e-07\r\nnum_events.sum()\r\n119\r\nApplication : Calculate OEP Curve\r\nHere I can now utilise the YLT to calculate Occurrency exceedance probability at any return periods I specify. The OEP is the probability that the associated loss level will be exceeded by any event in any given year.\r\nreturn_period = pd.DataFrame([10000,5000,1000,500,250,200,100,50, 25,10,5,2 ], columns=['return_period'])\r\nreturn_period['ntile'] = 1 - 1 / return_period['return_period'] \r\n\r\nprint(ylt['severity'].quantile(return_period['ntile']))\r\nntile\r\n0.9999    2147.938575\r\n0.9998    2146.353945\r\n0.9990    2133.676901\r\n0.9980    2117.830597\r\n0.9960    2086.137988\r\n0.9950    2070.291683\r\n0.9900    1873.766736\r\n0.9800    1599.245710\r\n0.9600    1362.202700\r\n0.9000     849.500856\r\n0.8000     547.204780\r\n0.5000      14.578265\r\nName: severity, dtype: float64\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-18-monte-carlo-simulation-in-python-part-1/riho-kroll-m4sGYaHYN5o-unsplash.jpg",
    "last_modified": "2020-12-18T21:16:43+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-13-testing-collapse/",
    "title": "data manipulation using collapse",
    "description": "An introduction to the collapse R package, testing and benchmarking some common data manipulation tasks against data.table and dplyr",
    "author": [
      {
        "name": "Randhir Bilkhu",
        "url": {}
      }
    ],
    "date": "2020-12-13",
    "categories": [
      "R",
      "packages",
      "data wrangling"
    ],
    "contents": "\r\n\r\nContents\r\nCreate a dummy dataset\r\nSummarising Data\r\nSubsetting data\r\nDeduplication\r\nConverting data types\r\nConclusion\r\nSession Info\r\n\r\n\r\n\r\n\r\nI recently came across collapse which is a C/C++ based package for data transformation and statistical computing in R. It offers a lot of functionality for statistical programming and time series analysis, and integrates well with the two main packages used for data manipulation in R, dplyr and data.table.\r\nMy focus in this post will be just to look at some basic data manipulation operations such as subsetting, recoding variables, de-duplicating and aggregating data to calculate summary statistics.\r\nI’ve written this primarily as a self-learning aid and I am sure there will likely be better ways to perform the operations or other features of collapse that are worth investigating. Corrections and suggestions would be welcome.\r\nCreate a dummy dataset\r\nFirst thing to do is to create a sample dataset so that we can test out the functions.\r\n\r\n\r\nlibrary(data.table)\r\nset.seed(1)\r\n\r\ntest_data <- data.table( ID = 1:10e6,\r\n                  values = sample(100:1000, size = 50, replace = TRUE),\r\n                  Code = sample(LETTERS[1:4], 10e6, replace = TRUE),\r\n                  City = rep(c(\"Tokyo\",\"Berlin\",\"Denver\",\"Stockholm\"), 10e6))\r\nsetkey(test_data, City)\r\n\r\n\r\n\r\nAs collapse is geared towards efficiency of operation I thought it best to use a fairly large dataset - this dataset has 40 million records.\r\n\r\nClasses 'data.table' and 'data.frame':  40000000 obs. of  4 variables:\r\n $ ID    : int  2 6 10 14 18 22 26 30 34 38 ...\r\n $ values: int  778 398 696 429 824 977 973 951 521 605 ...\r\n $ Code  : chr  \"C\" \"D\" \"B\" \"A\" ...\r\n $ City  : chr  \"Berlin\" \"Berlin\" \"Berlin\" \"Berlin\" ...\r\n - attr(*, \".internal.selfref\")=<externalptr> \r\n - attr(*, \"sorted\")= chr \"City\"\r\n\r\nSummarising Data\r\nA very common task is to group data by some categorical feature and then summarise it. Here I calculate the mean value for each city using the fmean function.\r\n\r\n\r\ncollapse::fmean(x= test_data$values, g= test_data$City)\r\n\r\n\r\n   Berlin    Denver Stockholm     Tokyo \r\n   633.56    546.44    633.56    546.44 \r\n\r\nWe can compare the performance of this function against a data.table and dplyr way of doing the same thing. The box plot below shows the results (please see the side note on Microbenchmark for details of how to produce this chart)\r\ndata.table is on average the best performing but collapse does provide some improvement on dplyr’s group by.\r\n\r\n\r\nmb <-microbenchmark (  \r\n        collapse  = collapse::fmean( x= test_data$values, g= test_data$City),\r\n        datatable = test_data[, .SD[,mean(values)]  , by=City],\r\n        dplyr     = test_data %>% dplyr::group_by(City) %>%  dplyr::summarize(mean(values)),\r\n        \r\n        times=100)\r\n\r\nplot_benchmark(mb)\r\n\r\n\r\n\r\n\r\nSubsetting data\r\nSubsetting or filtering the data based on some criteria is also a really common task. Here I used the fsubset function to filter the data only where the Code is equal to A.\r\n\r\n\r\ncollapse::fsubset(test_data, Code ==\"A\")\r\n\r\n\r\n               ID values Code   City\r\n       1:      14    429    A Berlin\r\n       2:      26    973    A Berlin\r\n       3:      30    951    A Berlin\r\n       4:      38    605    A Berlin\r\n       5:      46    636    A Berlin\r\n      ---                           \r\n10002424: 9999877    481    A  Tokyo\r\n10002425: 9999897    474    A  Tokyo\r\n10002426: 9999941    442    A  Tokyo\r\n10002427: 9999945    783    A  Tokyo\r\n10002428: 9999953    228    A  Tokyo\r\n\r\nAgain we can compare against data.table and dplyr - here data.table wins hands down, but collapse offers a significant improvement on dplyr.\r\n\r\n\r\nmb2 <- microbenchmark(\r\n   collapse = collapse::fsubset(test_data, Code ==\"A\"),\r\n   datatable = test_data[Code==\"A\",],\r\n   dplyr = test_data %>% dplyr::filter(Code==\"A\"),\r\n   times=10\r\n\r\n)\r\n\r\nplot_benchmark(mb2)\r\n\r\n\r\n\r\n\r\nDeduplication\r\nSometimes it is necessary to select only the unique values in a dataset. Collapse provides the ‘funique’ function which allows you to specify which columns are to be used for identifying unique rows in the dataset.\r\nHere I used fselect to select the City and Code columns and then funique to return the unique combinations of City and Code.\r\n\r\n\r\ntest_data %>% fselect(City, Code) %>% funique()\r\n\r\n\r\n         City Code\r\n 1:    Berlin    C\r\n 2:    Berlin    D\r\n 3:    Berlin    B\r\n 4:    Berlin    A\r\n 5:    Denver    B\r\n 6:    Denver    C\r\n 7:    Denver    D\r\n 8:    Denver    A\r\n 9: Stockholm    B\r\n10: Stockholm    C\r\n11: Stockholm    A\r\n12: Stockholm    D\r\n13:     Tokyo    C\r\n14:     Tokyo    D\r\n15:     Tokyo    A\r\n16:     Tokyo    B\r\n\r\n\r\n\r\nlibrary(collapse)\r\n\r\nmb3 <- microbenchmark(\r\n  collapse = test_data %>% fselect(City, Code) %>% funique(),\r\n  datatable = unique(test_data[, .(City, Code)]),\r\n  dplyr =  test_data %>% dplyr::select(City, Code) %>% dplyr::distinct(),\r\n  times=10\r\n)\r\n\r\nplot_benchmark(mb3)\r\n\r\n\r\n\r\n\r\nCollapse seems to be significantly better here.\r\nConverting data types\r\ncollapse offers a whole range of substitute functions for converting matrices, dataframes etc, Here I look at the converting a character vector to a factor.\r\n\r\n\r\nf1 <- collapse::qF(test_data$City)\r\n#you could replace f1 with test_data$City to recode the factor in the dataframe itself.\r\nstr(f1)\r\n\r\n\r\n Factor w/ 4 levels \"Berlin\",\"Denver\",..: 1 1 1 1 1 1 1 1 1 1 ...\r\n\r\nI compared this against the base method of doing this using as.factor. Collapse offers a siginificant performance improvement here over the base function.\r\n\r\n\r\nmb4 <- microbenchmark(\r\n  collapse = collapse::qF(test_data$City),\r\n  base = as.factor(test_data$City),\r\n  times=10\r\n  )\r\nplot_benchmark(mb4)\r\n\r\n\r\n\r\n\r\nIn practice I commonly use as.factor inside a data.table and use lapply to convert multiple fields. qF incorporates well in the same way. collapse provides a whole suite of re-coding functions (e.g. replacing NAs and Infs) which I think would be easy to incorporate with data.table or dplyr and clearly offer an improved level of performance.\r\n\r\n\r\nmb5 <- microbenchmark(\r\n   datatable_plus_base= test_data[, lapply(.SD, as.factor) , .SDcols = c(\"City\", \"Code\")],\r\n   datatable_plus_collapse = test_data[, lapply(.SD, collapse::qF) , .SDcols = c(\"City\", \"Code\")],\r\n   times=10\r\n\r\n)\r\n\r\nplot_benchmark(mb5)\r\n\r\n\r\n\r\n\r\nConclusion\r\nI think collapse is a great package with a whole array of functions which could be useful to improve the performance of data manipulation tasks, although in my opinion I would still rely on data.table/dplyr as the primary go to’s for handling data.\r\nOne thing I really like about collapse is that the %>% is incorporated and it integrates well with both dplyr and data.table. It might be a good option for those not familiar with data.table who want an improved performance.\r\nI have only touched the surface here as collapse offers a lot of additional statistical functionality which could be very useful especially when analysing financial data.\r\nI don’t think collapse will replace dplyr or data.table but I guess it’s not about either/or but about taking the best parts of every package and using them to improve your code. It’s definitely a package worth further investigation which I plan to do by trying out more of the functions and incorporating them into my workflow.\r\nSide Note on Microbenchmark\r\nThroughout this post I used the microbenchmark package to compare the performance of each operation. One of the aspects of microbenchmark I really like is that it comes with a default method to provide plot outputs of the results. This is in the form of a ggplot object which creates a violin plot.\r\nHowever, I’m not really a fan of the violin plots and wanted to change the default to a box plot. microbenchmark.autoplot is a S3 method which can easily be amended by using getS3method to return the underlying function and then modifying. Here I changed ggplot2::stat_ydensity() to ggplot2::geom_boxplot().\r\nThis post on Stack overflow was very helpful.\r\n\r\n\r\n#getS3method(\"autoplot\", \"microbenchmark\")\r\n\r\nfunction (object, ..., log = TRUE, y_max = 1.05 * max(object$time)) \r\n{\r\n    if (!requireNamespace(\"ggplot2\")) \r\n        stop(\"Missing package 'ggplot2'.\")\r\n    y_min <- 0\r\n    object$ntime <- convert_to_unit(object$time, \"t\")\r\n    plt <- ggplot2::ggplot(object, ggplot2::aes_string(x = \"expr\", \r\n        y = \"ntime\"))\r\n    plt <- plt + ggplot2::coord_cartesian(ylim = c(y_min, y_max))\r\n    plt <- plt + ggplot2::stat_ydensity()\r\n    plt <- plt + ggplot2::scale_x_discrete(name = \"\")\r\n    y_label <- sprintf(\"Time [%s]\", attr(object$ntime, \r\n        \"unit\"))\r\n    plt <- if (log) {\r\n        plt + ggplot2::scale_y_log10(name = y_label)\r\n    }\r\n    else {\r\n        plt + ggplot2::scale_y_continuous(name = y_label)\r\n    }\r\n    plt <- plt + ggplot2::coord_flip()\r\n    plt\r\n}\r\n\r\n# I changetd ggplot2::stat_ydensity() to geom_boxplot plus flipped the axees\r\nplot_benchmark <- function (object, ..., log = TRUE, y_max = 1.05 * max(object$time)) \r\n{\r\n    if (!requireNamespace(\"ggplot2\")) \r\n        stop(\"Missing package 'ggplot2'.\")\r\n    y_min <- 0\r\n    object$ntime <- microbenchmark:::convert_to_unit(object$time, \"t\")\r\n    plt <- ggplot2::ggplot(object, ggplot2::aes_string(x = \"expr\", \r\n        y = \"ntime\"))\r\n    plt <- plt + ggplot2::coord_cartesian(ylim = c(y_min, y_max))\r\n    plt <- plt + ggplot2::geom_boxplot()\r\n    plt <- plt + ggplot2::scale_x_discrete(name = \"\")\r\n    y_label <- sprintf(\"Time [%s]\", attr(object$ntime, \r\n        \"unit\"))\r\n    plt <- if (log) {\r\n        plt + ggplot2::scale_y_log10(name = y_label)\r\n    }\r\n    else {\r\n        plt + ggplot2::scale_y_continuous(name = y_label)\r\n    }\r\n    plt <- plt + ggplot2::coord_flip()\r\n    plt + theme_minimal_hgrid(12)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nSession Info\r\nsetting value\r\nversion R version 3.6.3 (2020-02-29) os Windows 10 x64\r\nsystem x86_64, mingw32\r\nui RStudio\r\nlanguage en_US.utf8\r\ndplyr_1.0.2\r\ncollapse_1.4.2\r\ndata.table_1.13.0\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-13-testing-collapse/sven-brandsma-OdF1YWzW_vA-unsplash.jpg",
    "last_modified": "2020-12-18T16:26:49+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-13-eltcat/",
    "title": "eltr",
    "description": "a package to help analyse outputs from catastrophe models in a reinsurance context",
    "author": [
      {
        "name": "Randhir Bilkhu",
        "url": {}
      }
    ],
    "date": "2020-11-20",
    "categories": [
      "R",
      "packages",
      "actuarial",
      "catastrophe models"
    ],
    "contents": "\r\nYou can find eltr on github\r\nMy goal with this package is to create functions which are helpful to analysing Catastrophe model outputs particularly in a re-insurance context.\r\nIntial documentation can be found here.\r\nThis first release is intended to be a foundation upon which to build. I have a pipeline of features to be added but would welcome ideas for development.\r\nPlease feel free to raise an issue here\r\nRelease Notes 0.1.0\r\ncreate_elt transforms a raw elt into a object with beta distribution parameters for secondary uncertainty\r\ncreate_ylt runs a monte carlo simulation assuming a poisson distribution for primary uncertainty\r\ncreate_oep_curve calculates exceedance probability and returns the occurrence loss at specified return periods\r\nlayer_loss calculates limited loss to layer\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-13-eltcat/hurr.jpg",
    "last_modified": "2020-12-21T16:58:18+00:00",
    "input_file": {}
  }
]
