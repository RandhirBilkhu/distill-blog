[
  {
    "path": "posts/2020-12-13-testing-collapse/",
    "title": "data manipulation using collapse",
    "description": "An introduction to the collapse R package, testing and benchmarking some common data manipulation tasks against data.table and dplyr",
    "author": [
      {
        "name": "Randhir Bilkhu",
        "url": {}
      }
    ],
    "date": "2020-12-13",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nCreate a dummy dataset\r\nSummarising Data\r\nSubsetting data\r\nDeduplication\r\nConverting data types\r\nConclusion\r\nSession Info\r\n\r\n\r\n\r\n\r\nI recently came across collapse which is a C/C++ based package for data transformation and statistical computing in R. It offers a lot of functionality for statistical programming and time series analysis, and integrates well with the two main packages used for data manipulation in R, dplyr and data.table.\r\nMy focus in this post will be just to look at some basic data manipulation operations such as subsetting, recoding variables, de-duplicating and aggregating data to calculate summary statistics.\r\nI’ve written this primarily as a self-learning aid and I am sure there will likely be better ways to perform the operations or other features of collapse that are worth investigating. Corrections and suggestions would be welcome.\r\nCreate a dummy dataset\r\nFirst thing to do is to create a sample dataset so that we can test out the functions.\r\n\r\n\r\nlibrary(data.table)\r\nset.seed(1)\r\n\r\ntest_data <- data.table( ID = 1:10e6,\r\n                  values = sample(100:1000, size = 50, replace = TRUE),\r\n                  Code = sample(LETTERS[1:4], 10e6, replace = TRUE),\r\n                  City = rep(c(\"Tokyo\",\"Berlin\",\"Denver\",\"Stockholm\"), 10e6))\r\nsetkey(test_data, City)\r\n\r\n\r\n\r\nAs collapse is geared towards efficiency of operation I thought it best to use a fairly large dataset - this dataset has 40 million records.\r\n\r\nClasses 'data.table' and 'data.frame':  40000000 obs. of  4 variables:\r\n $ ID    : int  2 6 10 14 18 22 26 30 34 38 ...\r\n $ values: int  778 398 696 429 824 977 973 951 521 605 ...\r\n $ Code  : chr  \"C\" \"D\" \"B\" \"A\" ...\r\n $ City  : chr  \"Berlin\" \"Berlin\" \"Berlin\" \"Berlin\" ...\r\n - attr(*, \".internal.selfref\")=<externalptr> \r\n - attr(*, \"sorted\")= chr \"City\"\r\n\r\nSummarising Data\r\nA very common task is to group data by some categorical feature and then summarise it. Here I calculate the mean value for each city using the fmean function.\r\n\r\n\r\ncollapse::fmean(x= test_data$values, g= test_data$City)\r\n\r\n\r\n   Berlin    Denver Stockholm     Tokyo \r\n   633.56    546.44    633.56    546.44 \r\n\r\nWe can compare the performance of this function against a data.table and dplyr way of doing the same thing. The box plot below shows the results (please see the side note on Microbenchmark for details of how to produce this chart)\r\ndata.table is on average the best performing but collapse does provide some improvement on dplyr’s group by.\r\n\r\n\r\nmb <-microbenchmark (  \r\n        collapse  = collapse::fmean( x= test_data$values, g= test_data$City),\r\n        datatable = test_data[, .SD[,mean(values)]  , by=City],\r\n        dplyr     = test_data %>% dplyr::group_by(City) %>%  dplyr::summarize(mean(values)),\r\n        \r\n        times=100)\r\n\r\nplot_benchmark(mb)\r\n\r\n\r\n\r\n\r\nSubsetting data\r\nSubsetting or filtering the data based on some criteria is also a really common task. Here I used the fsubset function to filter the data only where the Code is equal to A.\r\n\r\n\r\ncollapse::fsubset(test_data, Code ==\"A\")\r\n\r\n\r\n               ID values Code   City\r\n       1:      14    429    A Berlin\r\n       2:      26    973    A Berlin\r\n       3:      30    951    A Berlin\r\n       4:      38    605    A Berlin\r\n       5:      46    636    A Berlin\r\n      ---                           \r\n10002424: 9999877    481    A  Tokyo\r\n10002425: 9999897    474    A  Tokyo\r\n10002426: 9999941    442    A  Tokyo\r\n10002427: 9999945    783    A  Tokyo\r\n10002428: 9999953    228    A  Tokyo\r\n\r\nAgain we can compare against data.table and dplyr - here data.table wins hands down, but collapse offers a significant improvement on dplyr.\r\n\r\n\r\nmb2 <- microbenchmark(\r\n   collapse = collapse::fsubset(test_data, Code ==\"A\"),\r\n   datatable = test_data[Code==\"A\",],\r\n   dplyr = test_data %>% dplyr::filter(Code==\"A\"),\r\n   times=10\r\n\r\n)\r\n\r\nplot_benchmark(mb2)\r\n\r\n\r\n\r\n\r\nDeduplication\r\nSometimes it is necessary to select only the unique values in a dataset. Collapse provides the ‘funique’ function which allows you to specify which columns are to be used for identifying unique rows in the dataset.\r\nHere I used fselect to select the City and Code columns and then funique to return the unique combinations of City and Code.\r\n\r\n\r\ntest_data %>% fselect(City, Code) %>% funique()\r\n\r\n\r\n         City Code\r\n 1:    Berlin    C\r\n 2:    Berlin    D\r\n 3:    Berlin    B\r\n 4:    Berlin    A\r\n 5:    Denver    B\r\n 6:    Denver    C\r\n 7:    Denver    D\r\n 8:    Denver    A\r\n 9: Stockholm    B\r\n10: Stockholm    C\r\n11: Stockholm    A\r\n12: Stockholm    D\r\n13:     Tokyo    C\r\n14:     Tokyo    D\r\n15:     Tokyo    A\r\n16:     Tokyo    B\r\n\r\n\r\n\r\nlibrary(collapse)\r\n\r\nmb3 <- microbenchmark(\r\n  collapse = test_data %>% fselect(City, Code) %>% funique(),\r\n  datatable = unique(test_data[, .(City, Code)]),\r\n  dplyr =  test_data %>% dplyr::select(City, Code) %>% dplyr::distinct(),\r\n  times=10\r\n)\r\n\r\nplot_benchmark(mb3)\r\n\r\n\r\n\r\n\r\nCollapse seems to be significantly better here.\r\nConverting data types\r\ncollapse offers a whole range of substitute functions for converting matrices, dataframes etc, Here I look at the converting a character vector to a factor.\r\n\r\n\r\nf1 <- collapse::qF(test_data$City)\r\n#you could replace f1 with test_data$City to recode the factor in the dataframe itself.\r\nstr(f1)\r\n\r\n\r\n Factor w/ 4 levels \"Berlin\",\"Denver\",..: 1 1 1 1 1 1 1 1 1 1 ...\r\n\r\nI compared this against the base method of doing this using as.factor. Collapse offers a siginificant performance improvement here over the base function.\r\n\r\n\r\nmb4 <- microbenchmark(\r\n  collapse = collapse::qF(test_data$City),\r\n  base = as.factor(test_data$City),\r\n  times=10\r\n  )\r\nplot_benchmark(mb4)\r\n\r\n\r\n\r\n\r\nIn practice I commonly use as.factor inside a data.table and use lapply to convert multiple fields. qF incorporates well in the same way. collapse provides a whole suite of re-coding functions (e.g. replacing NAs and Infs) which I think would be easy to incorporate with data.table or dplyr and clearly offer an improved level of performance.\r\n\r\n\r\nmb5 <- microbenchmark(\r\n   datatable_plus_base= test_data[, lapply(.SD, as.factor) , .SDcols = c(\"City\", \"Code\")],\r\n   datatable_plus_collapse = test_data[, lapply(.SD, collapse::qF) , .SDcols = c(\"City\", \"Code\")],\r\n   times=10\r\n\r\n)\r\n\r\nplot_benchmark(mb5)\r\n\r\n\r\n\r\n\r\nConclusion\r\nI think collapse is a great package with a whole array of functions which could be useful to improve the performance of data manipulation tasks, although in my opinion I would still rely on data.table/dplyr as the primary go to’s for handling data.\r\nOne thing I really like about collapse is that the %>% is incorporated and it integrates well with both dplyr and data.table. It might be a good option for those not familiar with data.table who want an improved performance.\r\nI have only touched the surface here as collapse offers a lot of additional statistical functionality which could be very useful especially when analysing financial data.\r\nI don’t think collapse will replace dplyr or data.table but I guess it’s not about either/or but about taking the best parts of every package and using them to improve your code. It’s definitely a package worth further investigation which I plan to do by trying out more of the functions and incorporating them into my workflow.\r\nSide Note on Microbenchmark\r\nThroughout this post I used the microbenchmark package to compare the performance of each operation. One of the aspects of microbenchmark I really like is that it comes with a default method to provide plot outputs of the results. This is in the form of a ggplot object which creates a violin plot.\r\nHowever, I’m not really a fan of the violin plots and wanted to change the default to a box plot. microbenchmark.autoplot is a S3 method which can easily be amended by using getS3method to return the underlying function and then modifying. Here I changed ggplot2::stat_ydensity() to ggplot2::geom_boxplot().\r\nThis post on Stack overflow was very helpful.\r\n\r\n\r\n#getS3method(\"autoplot\", \"microbenchmark\")\r\n\r\nfunction (object, ..., log = TRUE, y_max = 1.05 * max(object$time)) \r\n{\r\n    if (!requireNamespace(\"ggplot2\")) \r\n        stop(\"Missing package 'ggplot2'.\")\r\n    y_min <- 0\r\n    object$ntime <- convert_to_unit(object$time, \"t\")\r\n    plt <- ggplot2::ggplot(object, ggplot2::aes_string(x = \"expr\", \r\n        y = \"ntime\"))\r\n    plt <- plt + ggplot2::coord_cartesian(ylim = c(y_min, y_max))\r\n    plt <- plt + ggplot2::stat_ydensity()\r\n    plt <- plt + ggplot2::scale_x_discrete(name = \"\")\r\n    y_label <- sprintf(\"Time [%s]\", attr(object$ntime, \r\n        \"unit\"))\r\n    plt <- if (log) {\r\n        plt + ggplot2::scale_y_log10(name = y_label)\r\n    }\r\n    else {\r\n        plt + ggplot2::scale_y_continuous(name = y_label)\r\n    }\r\n    plt <- plt + ggplot2::coord_flip()\r\n    plt\r\n}\r\n\r\n# I changetd ggplot2::stat_ydensity() to geom_boxplot plus flipped the axees\r\nplot_benchmark <- function (object, ..., log = TRUE, y_max = 1.05 * max(object$time)) \r\n{\r\n    if (!requireNamespace(\"ggplot2\")) \r\n        stop(\"Missing package 'ggplot2'.\")\r\n    y_min <- 0\r\n    object$ntime <- microbenchmark:::convert_to_unit(object$time, \"t\")\r\n    plt <- ggplot2::ggplot(object, ggplot2::aes_string(x = \"expr\", \r\n        y = \"ntime\"))\r\n    plt <- plt + ggplot2::coord_cartesian(ylim = c(y_min, y_max))\r\n    plt <- plt + ggplot2::geom_boxplot()\r\n    plt <- plt + ggplot2::scale_x_discrete(name = \"\")\r\n    y_label <- sprintf(\"Time [%s]\", attr(object$ntime, \r\n        \"unit\"))\r\n    plt <- if (log) {\r\n        plt + ggplot2::scale_y_log10(name = y_label)\r\n    }\r\n    else {\r\n        plt + ggplot2::scale_y_continuous(name = y_label)\r\n    }\r\n    plt <- plt + ggplot2::coord_flip()\r\n    plt + theme_minimal_hgrid(12)\r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nSession Info\r\nsetting value\r\nversion R version 3.6.3 (2020-02-29) os Windows 10 x64\r\nsystem x86_64, mingw32\r\nui RStudio\r\nlanguage en_US.utf8\r\ndplyr_1.0.2\r\ncollapse_1.4.2\r\ndata.table_1.13.0\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-13-testing-collapse/sven-brandsma-OdF1YWzW_vA-unsplash.jpg",
    "last_modified": "2020-12-16T10:09:53+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-13-eltcat/",
    "title": "eltr",
    "description": "a package to help analyse outputs from catastrophe models in a reinsurance context",
    "author": [
      {
        "name": "Randhir Bilkhu",
        "url": {}
      }
    ],
    "date": "2020-11-20",
    "categories": [
      "R",
      "packages"
    ],
    "contents": "\r\nFirst version of eltr released on github!\r\nMy goal with this package is to create functions which are helpful to analysing Catastrophe model outputs particularly in a re-insurance context.\r\nIntial documentation can be found here.\r\nThis first release is intended to be a foundation upon which to build. I have a pipeline of features to be added but would welcome ideas for development.\r\nPlease feel free to raise an issue here\r\nRelease Notes 0.1.0\r\ncreate_elt transforms a raw elt into a object with beta distribution parameters for secondary uncertainty\r\ncreate_ylt runs a monte carlo simulation assuming a poisson distribution for primary uncertainty\r\ncreate_oep_curve calculates exceedance probability and returns the occurrence loss at specified return periods\r\nlayer_loss calculates limited loss to layer\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-12-13-eltcat/hurr.jpg",
    "last_modified": "2020-12-16T10:13:04+00:00",
    "input_file": "eltcat.utf8.md"
  }
]
