{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Monte Carlo Simulation in Python - Part 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In order to help improve my use of Python, I decided to try an exercise to simulate loss events from an ELT ( Event loss table).\n",
    "\n",
    "Part 1 is about walking through the  steps to perform the calculation. Part 2 is about making the code reuseable and robust. The focus of these posts are more about the application of Python to achieve this task rather than the underlying theory. Interested readers can refer to [this](https://www.casact.org/pubs/forum/17spforumv2/02_Notes%20on%20Using%20Property%20Catastrophe%20Model%20Results.pdf)  paper by the Casulaty Actuarial society which explains a lot of the concepts.\n",
    "\n",
    "ELTS are a common output from the RMS property catstrophe model and usually contains the following:\n",
    "\n",
    "- Event ID :Unique identifier of the event\n",
    "- Rate : Annual event frequency\n",
    "- Mean : Average Loss if the event occurs\n",
    "- Sdi : Independent component of the spread of the loss if the event occurs.\n",
    "- Sdc : Correlated component of the spread of the loss if the event occurs.\n",
    "- Exposure : Total amount of limits exposed to the event (Maximum loss)\n",
    "\n",
    "We can use monte carlo techniques to sample events from this table and model loss occurences over N years-  to generate a YLT (Year Loss Table) which can then be used for analysis.\n",
    "\n",
    "#### High Level Process\n",
    "\n",
    "- generate a random sample of events by year assuming a poisson frequency\n",
    "- sample an event from the catalogue in the id and assign to the random sample\n",
    "- for each event generate a loss amount assuming a beta distribition\n",
    "- make some final modifications to tidy up the YLT\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Step 0 : Import the libraries\n",
    "\n",
    "All we need is pandas and numpy - an alternative would be scipy "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "source": [
    "#### Step1 : Load the data\n",
    "\n",
    "I have created a small sample ELT file - typically in reality there can thousands of catalogued events in the table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   id  rate   mean  sdevi  sdevc      exp\n0   1  0.10    500    500    200   100000\n1   2  0.10    200    400    100     5000\n2   3  0.20    300    200    400    40000\n3   4  0.10    100    300    500     4000\n4   5  0.20    500    100    200     2000\n5   6  0.25    200    200    500    50000\n6   7  0.01   1000    500    600   100000\n7   8  0.12    250    300    100     5000\n8   9  0.14   1000    500    200     6000\n9  10  0.00  10000   1000    500  1000000\n"
     ]
    }
   ],
   "source": [
    "elt = pd.read_csv(\"example.csv\")\n",
    "print(elt)"
   ]
  },
  {
   "source": [
    "Step 2 : Transfom the raw ELT\n",
    "\n",
    "We need to calculate some additional statistics which will be necessary in order to run the simulation, which can be done using pandas. \n",
    "\n",
    "I also set the index to start from 1 (Python by default starts from 0), firstly because I'm used to starting from one as an R user and secondly it will make some of the operations further on much easier to manage.  Note the syntax += which in python is equivalent to x = x + 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    id  rate   mean  sdevi  sdevc      exp       mdr  sdev   cov      alpha  \\\n1    1  0.10    500    500    200   100000  0.005000   700  1.40   0.508951   \n2    2  0.10    200    400    100     5000  0.040000   500  2.50   0.154589   \n3    3  0.20    300    200    400    40000  0.007500   600  2.00   0.248591   \n4    4  0.10    100    300    500     4000  0.025000   800  8.00   0.015240   \n5    5  0.20    500    100    200     2000  0.250000   300  0.60   6.818182   \n6    6  0.25    200    200    500    50000  0.004000   700  3.50   0.081333   \n7    7  0.01   1000    500    600   100000  0.010000  1100  1.10   0.825000   \n8    8  0.12    250    300    100     5000  0.050000   400  1.60   0.378486   \n9    9  0.14   1000    500    200     6000  0.166667   700  0.70   2.577320   \n10  10  0.00  10000   1000    500  1000000  0.010000  1500  0.15  79.200000   \n\n           beta  rand_num  \n1    101.281330  0.081967  \n2      3.710145  0.081967  \n3     32.896890  0.163934  \n4      0.594373  0.081967  \n5     20.454545  0.163934  \n6     20.251837  0.204918  \n7     81.675000  0.008197  \n8      7.191235  0.098361  \n9     12.886598  0.114754  \n10  7840.800000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "elt['mdr'] = elt['mean'] / elt['exp']  # calculates the mean damage ratio equivalent to average loss over total exposed\n",
    "elt['sdev'] = elt['sdevi'] + elt['sdevc'] # sums up the correlated and independent standard deviations \n",
    "elt['cov'] = elt['sdev'] /elt['mean'] # calculates covariance based on total standard deviation\n",
    "elt['alpha'] = (1 - elt['mdr']) / (elt['cov']**2 - elt['mdr']) # generates an alpha parameter for beta distribution\n",
    "#alpha is finite <-0 TODO\n",
    "elt['beta'] = (elt['alpha'] * (1 - elt['mdr'])) / elt['mdr']  # generates a beta parameter for beta distribution\n",
    "\n",
    "lda = elt['rate'].sum()  # total expected event frequency \n",
    "\n",
    "elt['rand_num'] = elt['rate'] / lda # probability of event occuring = normalised event frequency \n",
    "\n",
    "elt.index += 1 ### want to set index to start from 1 ( so sampling works)\n",
    "print(elt)"
   ]
  },
  {
   "source": [
    "#### Step 3 : Simulate Frequency of events by year\n",
    "\n",
    "We can run the simulation over 100 years.  This parameter will need to be much higher in practice in order to provide a reasonable YLT.  Especially when using ELTS for Earthquake losses which have very low probabilities of occurence.\n",
    "\n",
    "I assume frequency is a poisson random variable ( alternate would be negative binomial but would require an extra step to estimate the variance). We set the mean frequency to be equal to the sum of frequencies of each event and then use np.random.poisson to generate a number of events for each year\n",
    "\n",
    "random.seed() is useful to reproduce the data given by a pseudo-random number generator. By re-using a seed value, we can regenerate the same data multiple times as multiple threads are not running. This is helpful for reproducibility purposes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2 1 0 0 3 2 0 0 1 1 1 0 1 1 2 1 0 3 0 2 1 1 1 1 0 6 0 0 1 0 2 1 1 2 0 4 1\n 2 0 1 3 0 3 1 1 0 0 1 2 2 0 0 0 5 4 0 0 3 1 0 0 2 2 2 1 0 0 2 0 1 0 2 2 1\n 2 0 1 0 0 2 1 1 1 1 1 2 3 0 0 3 1 4 0 2 0 1 0 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "lda = elt['rate'].sum() ### total expected annual event frequency\n",
    "sims = 100  ## equivalent to number of years we will  simulate \n",
    "np.random.seed(42)\n",
    "num_events = np.random.poisson(lda, sims) \n",
    "print(num_events)"
   ]
  },
  {
   "source": [
    "Step 4: Draw an Event from the ELT for each event\n",
    "\n",
    "I used the normalised frequency to sample an id for each event.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "sample_ids = np.random.choice( a = elt['id'] , size = num_events.sum() , replace= True, p = elt['rand_num'] ) # for each occurrence we sample a loss event from the elt and create an array of ids\n",
    "\n",
    "len(sample_ids) "
   ]
  },
  {
   "source": [
    "#### Step 5 :  \n",
    "\n",
    "I create a new dataframe called sampled_event_loss which containst the alpha, beta parameters and amount exposed,for each event sampled. This required use of the `iloc` property in pandas which allows subsetting via reference ( the reference was the array of sampled ids)\n",
    "\n",
    "we can apply `random.beta` from numpy to sample a mean damage ratio (mdr) for each event from the distribution. Loss severity can be calculated by multiplying the sampled mdr with the amount exposed.\n",
    "\n",
    "Almost there with the YLT! Now just a few modifications are necessary.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    id     alpha       beta    exp  severity_mdr      severity\n3    3  0.248591  32.896890  40000  1.108744e-09  4.434975e-05\n6    6  0.081333  20.251837  50000  1.641959e-14  8.209797e-10\n6    6  0.081333  20.251837  50000  1.544267e-11  7.721333e-07\n8    8  0.378486   7.191235   5000  7.498051e-05  3.749025e-01\n6    6  0.081333  20.251837  50000  1.355502e-05  6.777512e-01\n..  ..       ...        ...    ...           ...           ...\n6    6  0.081333  20.251837  50000  4.924032e-07  2.462016e-02\n5    5  6.818182  20.454545   2000  2.377589e-01  4.755179e+02\n3    3  0.248591  32.896890  40000  4.575936e-04  1.830374e+01\n4    4  0.015240   0.594373   4000  6.785872e-22  2.714349e-18\n6    6  0.081333  20.251837  50000  1.918026e-11  9.590128e-07\n\n[116 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "sampled_event_loss = elt[['id', 'alpha','beta','exp']].iloc[elt.index.get_indexer(sample_ids)] ### this took some effort! \n",
    "sampled_event_loss['severity_mdr'] = sampled_event_loss.apply( lambda x: np.random.beta( x['alpha'] , x['beta']  ) , axis=1 ) ### use apply with axis =1 to use function on each row\n",
    "sampled_event_loss['severity'] = sampled_event_loss['severity_mdr'] * sampled_event_loss['exp'] ### this gives us severity for each event\n",
    "print(sampled_event_loss)\n"
   ]
  },
  {
   "source": [
    "#### Step 6: Allow for zero event years in the YLT\n",
    "\n",
    "The YLT needs to be adjusted for the years in which no loss occurred as this currently isnt being reflected in the table. We need to add a row for each year with no loss with 0 for severity. \n",
    "This will help ensure the YLT is accurate. This is a particularly important step when looking at perils with very low frequencies such as Earthquakes.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2, 1, 0, 0, 3, 2, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 0, 3, 0, 2, 1, 1,\n",
       "       1, 1, 0, 6, 0, 0, 1, 0, 2, 1, 1, 2, 0, 4, 1, 2, 0, 1, 3, 0, 3, 1,\n",
       "       1, 0, 0, 1, 2, 2, 0, 0, 0, 5, 4, 0, 0, 3, 1, 0, 0, 2, 2, 2, 1, 0,\n",
       "       0, 2, 0, 1, 0, 2, 2, 1, 2, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 2, 3, 0,\n",
       "       0, 3, 1, 4, 0, 2, 0, 1, 0, 2, 0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "num_events\n",
    "### have years with 0 events this will throw off exceedance probability calculations"
   ]
  },
  {
   "source": [
    "I used np.arange to generate a sequence from 1 to 100 for each year. Note by default the stop value is excluded hence the need for sims + 1. Next we create a dataframe from that array with the column name year.\n",
    "\n",
    "Finally we can use np.repeat which will create a feature in sampled_event_loss for the year in which each event occurred."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    id     alpha       beta    exp  severity_mdr      severity  year\n3    3  0.248591  32.896890  40000  1.108744e-09  4.434975e-05     1\n6    6  0.081333  20.251837  50000  1.641959e-14  8.209797e-10     1\n6    6  0.081333  20.251837  50000  1.544267e-11  7.721333e-07     2\n8    8  0.378486   7.191235   5000  7.498051e-05  3.749025e-01     5\n6    6  0.081333  20.251837  50000  1.355502e-05  6.777512e-01     5\n..  ..       ...        ...    ...           ...           ...   ...\n6    6  0.081333  20.251837  50000  4.924032e-07  2.462016e-02    94\n5    5  6.818182  20.454545   2000  2.377589e-01  4.755179e+02    96\n3    3  0.248591  32.896890  40000  4.575936e-04  1.830374e+01    98\n4    4  0.015240   0.594373   4000  6.785872e-22  2.714349e-18    98\n6    6  0.081333  20.251837  50000  1.918026e-11  9.590128e-07   100\n\n[116 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "year = np.arange(1, sims+1, 1) # start (included): 0, stop (excluded): 10, step:1\n",
    "all_years = pd.DataFrame(year , columns=['year'])\n",
    "\n",
    "sampled_event_loss['year'] = np.repeat(year, num_events)\n",
    "print(sampled_event_loss)"
   ]
  },
  {
   "source": [
    "Now we can create a dataframe YLT with just year and severity and use pd.merge to add in the years with 0 loss. Without using `fillna()` these will appear as NaN in the data frame\n",
    "\n",
    "We end up with a dataframe which shows the year in which loss occured and the loss amount. This can now be the basis for applying a reinsurance treaty structure and calculating layered losses. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     year      severity\n0       1  4.434975e-05\n1       1  8.209797e-10\n2       2  7.721333e-07\n3       3  0.000000e+00\n4       4  0.000000e+00\n5       5  3.749025e-01\n6       5  6.777512e-01\n7       5  5.491075e+02\n8       6  5.105934e+01\n9       6  6.427178e-19\n10      7  0.000000e+00\n11      8  0.000000e+00\n12      9  1.709651e+03\n13     10  1.384307e+01\n14     11  5.109882e+02\n15     12  0.000000e+00\n16     13  3.043422e-10\n17     14  1.191095e+03\n18     15  7.401779e+01\n19     15  3.821698e+02\n20     16  3.786217e+02\n21     17  0.000000e+00\n22     18  1.404651e-09\n23     18  4.645606e+02\n24     18  1.337478e+02\n25     19  0.000000e+00\n26     20  5.501849e-04\n27     20  6.297897e+01\n28     21  2.155536e+02\n29     22  9.456517e+01\n30     23  6.318721e-02\n31     24  1.463509e+03\n32     25  0.000000e+00\n33     26  4.000737e+02\n34     26  6.542851e+02\n35     26  4.995096e-14\n36     26  1.531346e+01\n37     26  8.527407e+02\n38     26  6.277614e+02\n39     27  0.000000e+00\n40     28  0.000000e+00\n41     29  1.025373e+03\n42     30  0.000000e+00\n43     31  8.203426e+02\n44     31  6.533643e+01\n45     32  7.202268e+00\n46     33  3.569292e-19\n47     34  6.258983e+01\n48     34  4.648111e-02\n49     35  0.000000e+00\n50     36  2.446549e+02\n51     36  5.153527e+02\n52     36  1.599666e+03\n53     36  2.142401e-07\n54     37  3.825918e+02\n55     38  1.231233e-05\n56     38  1.290300e+03\n57     39  0.000000e+00\n58     40  4.572607e+02\n59     41  2.817630e-01\n60     41  5.395940e+02\n61     41  5.805758e+02\n62     42  0.000000e+00\n63     43  4.282555e+02\n64     43  1.033512e-05\n65     43  8.384080e-03\n66     44  1.071078e-26\n67     45  6.705483e+02\n68     46  0.000000e+00\n69     47  0.000000e+00\n70     48  1.174639e+03\n71     49  3.909565e+01\n72     49  6.985921e-01\n73     50  4.954364e+02\n74     50  6.822492e+02\n75     51  0.000000e+00\n76     52  0.000000e+00\n77     53  0.000000e+00\n78     54  3.839980e+02\n79     54  7.611154e+01\n80     54  1.545606e-14\n81     54  3.325711e-04\n82     54  9.315729e+00\n83     55  7.976122e+01\n84     55  4.177383e-36\n85     55  8.632855e+00\n86     55  2.856225e+01\n87     56  0.000000e+00\n88     57  0.000000e+00\n89     58  1.293269e+00\n90     58  1.234734e+02\n91     58  6.529987e+02\n92     59  5.766023e+02\n93     60  0.000000e+00\n94     61  0.000000e+00\n95     62  4.892098e+01\n96     62  3.527225e+00\n97     63  4.131111e+02\n98     63  8.828718e+01\n99     64  8.390815e-01\n100    64  2.293760e-23\n101    65  5.391410e+02\n102    66  0.000000e+00\n103    67  0.000000e+00\n104    68  6.567555e+02\n105    68  3.015369e+00\n106    69  0.000000e+00\n107    70  3.573933e+01\n108    71  0.000000e+00\n109    72  6.060894e-09\n110    72  3.794436e+01\n111    73  1.055675e+02\n112    73  3.697382e+02\n113    74  6.461396e+02\n114    75  2.149523e+03\n115    75  1.141927e+03\n116    76  0.000000e+00\n117    77  7.663019e+02\n118    78  0.000000e+00\n119    79  0.000000e+00\n120    80  4.003370e+02\n121    80  9.739867e+01\n122    81  1.471702e+02\n123    82  1.182795e+01\n124    83  5.548184e+02\n125    84  2.044581e+03\n126    85  7.969508e+02\n127    86  9.489739e+02\n128    86  1.785455e+01\n129    87  5.495236e+02\n130    87  9.719987e-60\n131    87  3.351133e-01\n132    88  0.000000e+00\n133    89  0.000000e+00\n134    90  1.436955e+02\n135    90  3.219530e+02\n136    90  4.486940e+02\n137    91  1.002510e+03\n138    92  5.628708e-01\n139    92  1.236078e+03\n140    92  1.578653e+03\n141    92  1.365199e+03\n142    93  0.000000e+00\n143    94  4.649067e+02\n144    94  2.462016e-02\n145    95  0.000000e+00\n146    96  4.755179e+02\n147    97  0.000000e+00\n148    98  1.830374e+01\n149    98  2.714349e-18\n150    99  0.000000e+00\n151   100  9.590128e-07\n"
     ]
    }
   ],
   "source": [
    "ylt = sampled_event_loss[['year', 'severity']]\n",
    "ylt = pd.merge(ylt, all_years, how='right').fillna(0)\n",
    " \n",
    "print(ylt.to_string()) # allows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "num_events.sum()"
   ]
  },
  {
   "source": [
    "#### Application : Calculate OEP Curve\n",
    "\n",
    "Here I can now utilise the YLT to calculate Occurrency exceedance probability at any return periods I specify. The OEP is the probability that the associated loss level will be exceeded by any event in any given year."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ntile\n0.9999    2147.938575\n0.9998    2146.353945\n0.9990    2133.676901\n0.9980    2117.830597\n0.9960    2086.137988\n0.9950    2070.291683\n0.9900    1873.766736\n0.9800    1599.245710\n0.9600    1362.202700\n0.9000     849.500856\n0.8000     547.204780\n0.5000      14.578265\nName: severity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "return_period = pd.DataFrame([10000,5000,1000,500,250,200,100,50, 25,10,5,2 ], columns=['return_period'])\n",
    "return_period['ntile'] = 1 - 1 / return_period['return_period'] \n",
    "\n",
    "print(ylt['severity'].quantile(return_period['ntile']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'commit_hash': '033ab93c7',\n 'commit_source': 'installation',\n 'default_encoding': 'utf-8',\n 'ipython_path': 'C:\\\\Users\\\\randz\\\\AppData\\\\Local\\\\r-miniconda\\\\envs\\\\r-reticulate\\\\lib\\\\site-packages\\\\IPython',\n 'ipython_version': '5.8.0',\n 'os_name': 'nt',\n 'platform': 'Windows-10-10.0.19041-SP0',\n 'sys_executable': 'C:\\\\Users\\\\randz\\\\AppData\\\\Local\\\\r-miniconda\\\\envs\\\\r-reticulate\\\\python.exe',\n 'sys_platform': 'win32',\n 'sys_version': '3.6.11 (default, Nov 27 2020, 18:37:51) [MSC v.1916 64 bit '\n                '(AMD64)]'}\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "print(IPython.sys_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}